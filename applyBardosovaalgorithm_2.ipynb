{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ujson igraph xnetwork infomap"
      ],
      "metadata": {
        "id": "qvy6UywIdXoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDQqIwuiYNzm"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "from os.path import join as PJ\n",
        "# import bgzf\n",
        "import struct\n",
        "import numpy as np\n",
        "import operator\n",
        "import gensim\n",
        "import ujson\n",
        "import igraph as ig\n",
        "import xnetwork as xn\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from os.path import join as PJ\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmeZiY73YNzo"
      },
      "outputs": [],
      "source": [
        "from infomap import Infomap\n",
        "def infomapMembership(vertexCount,edges):\n",
        "    im = Infomap(\"-N 10 --ftree --silent --seed %d\"%np.random.randint(4294967296, dtype=np.int64));\n",
        "    im.setVerbosity(0);\n",
        "    for nodeIndex in range(0,vertexCount):\n",
        "        im.add_node(nodeIndex)\n",
        "    for edge in edges:\n",
        "        im.add_link(edge[0], edge[1]);\n",
        "    im.run()\n",
        "    # print(\"Result\")\n",
        "    # print(\"\\n#node module\")\n",
        "    membership = [0]*vertexCount;\n",
        "    for node in im.tree:\n",
        "        if node.is_leaf:\n",
        "            membership[node.node_id] = node.module_id;\n",
        "    return membership"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVFLWBcxYNzo"
      },
      "outputs": [],
      "source": [
        "def infomapApply(g, weights=None):\n",
        "    vertexCount = g.vcount()\n",
        "    if(weights):\n",
        "        edges = [(e.source, e.target, e[weights]) for e in g.es]\n",
        "    else:\n",
        "        edges = g.get_edgelist()\n",
        "\n",
        "#     if(g.is_directed()):\n",
        "#         extraOptions = \"-d\"\n",
        "#     else:\n",
        "    extraOptions = \"\"\n",
        "    im = Infomap(\"%s -N 100 --silent --seed %d\" %\n",
        "                 (extraOptions, np.random.randint(4294967296, dtype=np.int64)), markov_time=2.0) # markov_time = Default 1\n",
        "\n",
        "    im.setVerbosity(0)\n",
        "    for nodeIndex in range(0, vertexCount):\n",
        "        im.add_node(nodeIndex)\n",
        "    for edge in edges:\n",
        "        if(len(edge) > 2):\n",
        "            if(edge[2]>0):\n",
        "                im.addLink(edge[0], edge[1], edge[2])\n",
        "            im.add_link(edge[0], edge[1], weight=edge[2])\n",
        "        else:\n",
        "            im.add_link(edge[0], edge[1])\n",
        "\n",
        "    im.run()\n",
        "    membership = [\":\".join([str(a) for a in membership])\n",
        "                  for index, membership in im.get_multilevel_modules().items()]\n",
        "\n",
        "    levelMembership = []\n",
        "    levelCount = max([len(element.split(\":\")) for element in membership])\n",
        "    for level in range(levelCount):\n",
        "        print(level)\n",
        "        levelMembership.append(\n",
        "            [\":\".join(element.split(\":\")[:(level+1)]) for element in membership]\n",
        "        )\n",
        "    return levelMembership"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE8NbKzvYNzo"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords;\n",
        "from nltk.stem.wordnet import WordNetLemmatizer;\n",
        "import nltk.data;\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
        "from nltk.corpus import wordnet;\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "verboseMode = True;\n",
        "\n",
        "# Loading manual dictionary an ignore list\n",
        "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
        "replaceDictionary = {};\n",
        "# with open(\"replaceDictionary.dat\",\"r\") as fp:\n",
        "# \tfor line in fp:\n",
        "# \t\tentry = line.strip().split(\"\\t\");\n",
        "# \t\tif(len(entry)>1):\n",
        "# \t\t\treplaceDictionary[entry[0]] = entry[1];\n",
        "\n",
        "# ignoreSet = set();\n",
        "# with open(\"ignoreSet.dat\",\"r\") as fp:\n",
        "# \tfor line in fp:\n",
        "# \t\tignoreSet.add(line.strip());\n",
        "\n",
        "\n",
        "#Setting up nltk environment\n",
        "if(verboseMode): print(\"Setting up nltk environment.\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKoi8SFnYNzo"
      },
      "outputs": [],
      "source": [
        "%load_ext Cython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMAaxaqxYNzo"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ_hZHymYNzp"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cj41nZJYNzp"
      },
      "outputs": [],
      "source": [
        "%%cython\n",
        "from nltk.corpus import stopwords;\n",
        "from nltk.stem.wordnet import WordNetLemmatizer;\n",
        "import nltk.data;\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize;\n",
        "from nltk.corpus import wordnet;\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "lmtzr = WordNetLemmatizer();\n",
        "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "punctuation = re.compile(r'[(\\])(\\})(\\{)(\\[).?!,\":;()|<>]/ < >');\n",
        "stopSet = set(stopwords.words('english'));\n",
        "\n",
        "print(stopSet)\n",
        "\n",
        "verboseMode = True;\n",
        "\n",
        "# Loading manual dictionary an ignore list\n",
        "if(verboseMode): print(\"Loading manual dictionary an ignore list.\");\n",
        "replaceDictionary = {};\n",
        "with open(\"replaceDictionary.txt\",\"r\") as fp:\n",
        "\tfor line in fp:\n",
        "\t\tentry = line.strip().split(\"\\t\");\n",
        "\t\tif(len(entry)>1):\n",
        "\t\t\treplaceDictionary[entry[0]] = entry[1];\n",
        "\n",
        "ignoreSet = set();\n",
        "with open(\"ignoreSet.txt\",\"r\") as fp:\n",
        "\tfor line in fp:\n",
        "\t\tignoreSet.add(line.strip());\n",
        "\n",
        "\n",
        "#Setting up nltk environment\n",
        "if(verboseMode): print(\"Setting up nltk environment.\");\n",
        "\n",
        "\n",
        "def findWholeWord(w):\n",
        "\treturn re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\tif treebank_tag.startswith('J'):\n",
        "\t\treturn wordnet.ADJ\n",
        "\telif treebank_tag.startswith('V'):\n",
        "\t\treturn wordnet.VERB\n",
        "\telif treebank_tag.startswith('N'):\n",
        "\t\treturn wordnet.NOUN\n",
        "\telif treebank_tag.startswith('R'):\n",
        "\t\treturn wordnet.ADV\n",
        "\telse:\n",
        "\t\treturn ''\n",
        "\n",
        "# def get_wordnet_pos(treebank_tag):\n",
        "# \tif treebank_tag.startswith('J'):\n",
        "# \t\treturn -1\n",
        "# \telif treebank_tag.startswith('V'):\n",
        "# \t\treturn -1\n",
        "# \telif treebank_tag.startswith('N'):\n",
        "# \t\treturn wordnet.NOUN\n",
        "# \telif treebank_tag.startswith('R'):\n",
        "# \t\treturn wordnet.ADV\n",
        "# \telse:\n",
        "# \t\treturn ''\n",
        "\n",
        "#Setting up tokenizer\n",
        "if(verboseMode): print(\"Setting up tokenizer.\");\n",
        "\n",
        "tokenizerInput = {\n",
        "\t\"stopSet\":stopSet,\n",
        "\t\"punctuation\":punctuation,\n",
        "\t\"tokenizer\":sent_tokenizer,\n",
        "\t\"lematizer\":lmtzr,\n",
        "\t\"sent_tokenize\": sent_tokenize,\n",
        "\t\"replaceDictionary\": replaceDictionary,\n",
        "\t\"ignoreSet\":ignoreSet\n",
        "}\n",
        "\n",
        "def tokenizeString(theString,maximumTokenSize,tokenizerInput,removeStopWords=True):\n",
        "\tstopSet = tokenizerInput[\"stopSet\"];\n",
        "\tlematizer = tokenizerInput[\"lematizer\"];\n",
        "\ttokenizer = tokenizerInput[\"tokenizer\"];\n",
        "\tpunctuation = tokenizerInput[\"punctuation\"];\n",
        "\tsent_tokenize = tokenizerInput[\"sent_tokenize\"];\n",
        "\treplaceDictionary = tokenizerInput[\"replaceDictionary\"];\n",
        "\tignoreSet = tokenizerInput[\"ignoreSet\"];\n",
        "\twordsList = [];\n",
        "\ttitleAbstract = (\". \".join(theString.split(\"::\"))).strip();\n",
        "\twordsSentences = [word_tokenize(t) for t in sent_tokenize(titleAbstract)];\n",
        "\tstopSentence = False;\n",
        "\tfor si, words in enumerate(wordsSentences):\n",
        "\t\twordsTags = nltk.pos_tag(words);\n",
        "\t\tif(stopSentence):\n",
        "\t\t\tbreak;\n",
        "\t\tfor wi,wordTag in enumerate(wordsTags):\n",
        "\t\t\tword = wordTag[0];\n",
        "\t\t\ttag = wordTag[1];\n",
        "\n",
        "\t\t\tif word.isdigit() or word[1:].isdigit():\n",
        "\t\t\t\tcontinue;\n",
        "# \t\t\tif(si>len(wordsSentences)-4 and (word.lower()==\"copyright\" or (wi>0 and word.lower()==\"c\" and words[wi-1] == \"(\"  and words[wi+1] == \")\" ))):\n",
        "# \t\t\t\tstopSentence = True;\n",
        "# \t\t\t\tbreak;\n",
        "\t\t\tword = punctuation.sub(\"\", word);\n",
        "\t\t\tconvTag = get_wordnet_pos(tag);\n",
        "\t\t\t#print \"w: \"+word;\n",
        "\t\t\tif convTag == -1:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tif(convTag != ''):\n",
        "\t\t\t\tword  = lematizer.lemmatize(word.lower(), convTag);\n",
        "\t\t\telse:\n",
        "\t\t\t\tword  = lematizer.lemmatize(word.lower());\n",
        "\t\t\tif(len(word)==0 or ((word in stopSet) and removeStopWords) or (word in ignoreSet)):\n",
        "\t\t\t\tcontinue;\n",
        "\t\t\telse:\n",
        "\t\t\t\tif(word in replaceDictionary):\n",
        "\t\t\t\t\twordsList.append(replaceDictionary[word]);\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\twordsList.append(word);\n",
        "\n",
        "\ttokens = [set() for i in range(maximumTokenSize)];\n",
        "\tfor wordIndex in range(len(wordsList)):\n",
        "\t\tfor tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
        "\t\t\ttokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
        "\treturn tokens;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR8pw1HaYNzp"
      },
      "outputs": [],
      "source": [
        " # important to level ifomap apply\n",
        "\n",
        "def convert_membership_to_indices(membership_vector):\n",
        "    groups_array = np.array(membership_vector)\n",
        "    _, membership_indices = np.unique(groups_array, return_inverse=True)\n",
        "    return membership_indices.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owniKlcbYNzp"
      },
      "outputs": [],
      "source": [
        "def apply_bardosova(network, jsonFileprefix):\n",
        "\n",
        "    removeStopWords = True;\n",
        "    maximumTokenSize = 3; #n-gram\n",
        "    minKeywordsPerCluster = 10;\n",
        "    maxKeywordsPerCluster = 10;\n",
        "    maxClusterNameLength = 150;\n",
        "    useMajorComponent = True;\n",
        "    verboseMode = True;\n",
        "\n",
        "    # Obtaining the major connected component (if needed)\n",
        "    if(useMajorComponent):\n",
        "        if(verboseMode): print(\"Obtaining the major connected component.\");\n",
        "        network = network.clusters(\"WEAK\").giant();\n",
        "\n",
        "    # Tokenizing the abstracts\n",
        "    if(verboseMode): print(\"Tokenizing the abstracts.\");\n",
        "    tokensFrequency = [[] for i in range(maximumTokenSize)];\n",
        "    tokensGroupFrequency = [{} for i in range(maximumTokenSize)];\n",
        "\n",
        "    propertiesKeys = set();\n",
        "\n",
        "    verticesTokens = [];\n",
        "    for vertexIndex in range(network.vcount()):\n",
        "        if(vertexIndex%100==0):\n",
        "            print(\"Tokenizing: %d/%d             \"%(vertexIndex,network.vcount()),end=\"\\r\")\n",
        "\n",
        "    #         for wordsList in tokenList:\n",
        "    #             tokens = [set() for i in range(maximumTokenSize)];\n",
        "    #             for wordIndex in range(len(wordsList)):\n",
        "    #                 for tokenSize in range(0,min(wordIndex+1,maximumTokenSize)):\n",
        "    #                     tokens[tokenSize].add(\" \".join(wordsList[(wordIndex-tokenSize):(wordIndex+1)]));\n",
        "        verticesTokens.append(tokenizeString(network.vs[vertexIndex][\"title\"],maximumTokenSize,tokenizerInput));\n",
        "\n",
        "    print(\"Done                   \");\n",
        "\n",
        "    # Obtaining the network community structure\n",
        "    if(verboseMode): print(\"Obtaining the network community structure.\");\n",
        "\n",
        "\n",
        "    edgelist = [(e.source,e.target) for e in network.es]\n",
        "    communities = infomapApply(network)[0]\n",
        "    # communities = [int(c) for c in communities]\n",
        "    communities = convert_membership_to_indices(communities)\n",
        "    print()\n",
        "\n",
        "    # print(\"Modularity: %f\"%cc.q);\n",
        "\n",
        "    clusters = [[] for i in range(max(communities)+1)];\n",
        "    for vertexIndex in range(network.vcount()):\n",
        "        clusters[communities[vertexIndex]].append(vertexIndex);\n",
        "\n",
        "    #sorting the clusters by size\n",
        "    clusters = sorted(clusters,key=len,reverse=True);\n",
        "\n",
        "    # Getting tokens frequency\n",
        "    if(verboseMode): print(\"Getting tokens frequency.\");\n",
        "\n",
        "\n",
        "    tokenFrequencyInClusters = [];\n",
        "    tokenFrequencyInCorpus = {};\n",
        "\n",
        "    for clusterIndex in range(len(clusters)):\n",
        "        cluster = clusters[clusterIndex];\n",
        "        tokenFrequencyInCluster = {};\n",
        "        for vertexIndex in cluster:\n",
        "            tokens = verticesTokens[vertexIndex];\n",
        "            for tokenSize in range(0,maximumTokenSize):\n",
        "                for token in tokens[tokenSize]:\n",
        "                    if(token not in tokenFrequencyInCorpus):\n",
        "                        tokenFrequencyInCorpus[token] = 0;\n",
        "                    if(token not in tokenFrequencyInCluster):\n",
        "                        tokenFrequencyInCluster[token] = 0;\n",
        "                    tokenFrequencyInCorpus[token] += 1;\n",
        "                    tokenFrequencyInCluster[token] += 1;\n",
        "        tokenFrequencyInClusters.append(tokenFrequencyInCluster);\n",
        "\n",
        "    # Calculating the importance Index\n",
        "    if(verboseMode): print(\"Calculating the importance Index.\");\n",
        "    #tokenRelativeFrequencyInClusters = [];\n",
        "    #tokenRelativeFrequencyOutClusters = [];\n",
        "    tokenImportanceIndexInClusters = [];\n",
        "\n",
        "    verticesCount = network.vcount();\n",
        "    for clusterIndex in range(len(clusters)):\n",
        "        clusterSize = len(clusters[clusterIndex]);\n",
        "\n",
        "        tokenFrequencyInCluster = tokenFrequencyInClusters[clusterIndex];\n",
        "\n",
        "        #tokenRelativeFrequencyInCluster = {};\n",
        "        #tokenRelativeFrequencyOutCluster = {};\n",
        "        tokenImportanceIndexInCluster = {};\n",
        "\n",
        "        for token in tokenFrequencyInCluster:\n",
        "            nInCluster = tokenFrequencyInCluster[token];\n",
        "            nOutCluster = tokenFrequencyInCorpus[token]-nInCluster;\n",
        "            outClusterSize = verticesCount-clusterSize;\n",
        "            if(nOutCluster==0):\n",
        "                outClusterSize = 1; #Fix for singletons\n",
        "            FInCluster = float(nInCluster)/float(clusterSize);\n",
        "            FOutCluster = float(nOutCluster)/float(outClusterSize);\n",
        "            importanceIndex = FInCluster-FOutCluster;\n",
        "            #tokenRelativeFrequencyInCluster[token] = FInCluster;\n",
        "            #tokenRelativeFrequencyOutCluster[token] = FOutCluster;\n",
        "            tokenImportanceIndexInCluster[token] = importanceIndex;\n",
        "\n",
        "        #tokenRelativeFrequencyInClusters.append(tokenRelativeFrequencyInCluster);\n",
        "        #tokenRelativeFrequencyOutClusters.append(tokenRelativeFrequencyOutCluster);\n",
        "        tokenImportanceIndexInClusters.append(tokenImportanceIndexInCluster);\n",
        "\n",
        "    defaultNames = \"ABCDEFGHIJKLMNOPQRSTUWVXYZ\";\n",
        "    defaultNamesLength = len(defaultNames);\n",
        "\n",
        "    clusterKeywords = [];\n",
        "    minClusterSize = min([len(cluster) for cluster in clusters]);\n",
        "    maxClusterSize = max([len(cluster) for cluster in clusters]);\n",
        "    clusterNames = [];\n",
        "    for clusterIndex in range(len(clusters)):\n",
        "        cluster = clusters[clusterIndex];\n",
        "        clusterSize = len(cluster);\n",
        "        keywords = [v[0] for v in sorted(tokenImportanceIndexInClusters[clusterIndex].items(),key=operator.itemgetter(1),reverse=True)];\n",
        "        if(maxClusterSize>minClusterSize):\n",
        "            m = (maxKeywordsPerCluster-minKeywordsPerCluster)/float(maxClusterSize-minClusterSize);\n",
        "        else:\n",
        "            m=0;\n",
        "        keywordsCount = round(m*(clusterSize-minClusterSize)+minKeywordsPerCluster);\n",
        "        currentKeywords = [];\n",
        "        while(len(currentKeywords)<keywordsCount and len(keywords)>len(currentKeywords)):\n",
        "            currentKeywords = keywords[0:keywordsCount];\n",
        "            jointKeywords = \".\"+\".\".join(currentKeywords)+\".\";\n",
        "            toRemoveKeywords = [];\n",
        "            for keyword in currentKeywords:\n",
        "                if(jointKeywords.find(\" %s.\"%keyword)>=0):\n",
        "                    toRemoveKeywords.append(keyword);\n",
        "                elif(jointKeywords.find(\".%s \"%keyword)>=0):\n",
        "                    toRemoveKeywords.append(keyword);\n",
        "            for toRemoveKeyword in toRemoveKeywords:\n",
        "                keywords.remove(toRemoveKeyword);\n",
        "                currentKeywords.remove(toRemoveKeyword);\n",
        "        clusterKeywords.append(currentKeywords);\n",
        "        #print(currentKeywords);\n",
        "        clusterName = \"\";\n",
        "        if(clusterIndex<defaultNamesLength):\n",
        "            clusterName += defaultNames[clusterIndex];\n",
        "        else:\n",
        "            clusterName += \"{%d}\"%(clusterIndex);\n",
        "        clusterName += \" - \"+\", \".join(currentKeywords);\n",
        "        if(len(clusterName)>maxClusterNameLength):\n",
        "            clusterName = clusterName[0:maxClusterNameLength-1]+\"...\";\n",
        "        for vertexIndex in cluster:\n",
        "            network.vs[vertexIndex][\"ClusterName\"] = clusterName;\n",
        "            network.vs[vertexIndex][\"ClusterIndex\"] = clusterIndex;\n",
        "        clusterNames.append(clusterName);\n",
        "        print(clusterName);\n",
        "        print(clusterIndex);\n",
        "        print(currentKeywords);\n",
        "\n",
        "\n",
        "    # Saving the network\n",
        "    if(verboseMode): print(\"Saving the network.\");\n",
        "    # network.vs[\"kcore\"] = network.coreness()\n",
        "\n",
        "    xn.igraph2xnet(network,fileName=PJ('',\"%s_infomap.xnet\"%(jsonFileprefix)),ignoredNodeAtts=[\"Text\"]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD4owZqMYNzp"
      },
      "outputs": [],
      "source": [
        "file = './Langmuir_Blodgett_films.xnet'\n",
        "network = xn.xnet2igraph(file)\n",
        "network.vs['wos_id'] = network.vs['name']\n",
        "network.vs['name'] = network.vs['title']\n",
        "\n",
        "print(network.vs['name'][:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fl_-1m2YNzp"
      },
      "outputs": [],
      "source": [
        "output_header = 'Langmuir_Blodgett_films'\n",
        "apply_bardosova(network, output_header)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-GWIwx5cwO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}